# ğŸ“˜ Studiu de Caz â€“ Regularizare Ã®n Modele de Regressie  
**Elastic Net vs. Ridge vs. Lasso**  
*(include È™i experimentul suplimentar cu validare temporalÄƒ)*

Acest repository conÈ›ine implementarea unui studiu comparativ al metodelor de regularizare folosite pentru reducerea overfitting-ului Ã®n modelele de regresie, aplicate pe date reale din proiectul **Beijing Multi-Site Air Quality**.

Scopul proiectului este evaluarea impactului tehnicilor **L1 (Lasso)**, **L2 (Ridge)** È™i **Elastic Net** asupra performanÈ›ei, stabilitÄƒÈ›ii coeficienÈ›ilor È™i capacitÄƒÈ›ii de generalizare.

---

## ğŸ§  ConÈ›inutul proiectului

### âœ” Pre-procesarea setului de date
- Ã®ncÄƒrcarea È™i curÄƒÈ›area datelor brute  
- completarea valorilor lipsÄƒ (numeric/categoric)  
- One-Hot Encoding pentru direcÈ›ia vÃ¢ntului  
- scalare standardizatÄƒ  
- transformarea logaritmicÄƒ a PM2.5  
- Ã®mpÄƒrÈ›irea Train/Test menÈ›inÃ¢nd ordinea temporalÄƒ  

### âœ” Modelele implementate
- **Linear Regression** (baseline)  
- **Ridge Regression (L2)**  
- **Lasso Regression (L1)**  
- **Elastic Net (L1+L2)**  

Optimizarea hiperparametrilor a fost realizatÄƒ atÃ¢t cu **GridSearchCV (K-Fold)**, cÃ¢t È™i Ã®ntr-un **experiment suplimentar** folosind **TimeSeriesSplit**, metoda corectÄƒ pentru date temporale.

### âœ” AnalizÄƒ È™i vizualizÄƒri
- RMSE & RÂ² (Train/Test)  
- analizÄƒ biasâ€“varianÈ›Äƒ  
- curbe de Ã®nvÄƒÈ›are  
- distribuÈ›ia predicÈ›iilor (KDE)  
- comparaÈ›ia coeficienÈ›ilor + coeficienÈ›i eliminaÈ›i  
- interpretarea modelului Elastic Net prin **SHAP**  

---

## ğŸ“Š Rezultate principale

### ğŸ”¹ Rezultatele iniÈ›iale (validare K-Fold)

| Model              | RMSE Test | ObservaÈ›ii                                      |
|--------------------|-----------|--------------------------------------------------|
| Linear Regression  | 0.5047    | Precizie numericÄƒ ridicatÄƒ, varianÈ›Äƒ uÈ™or mare  |
| Lasso (L1)         | ~0.505    | EliminÄƒ puÈ›ine variabile, regularizare slabÄƒ     |
| Ridge (L2)         | ~0.506    | Cel mai stabil Ã®n prezenÈ›a multicoliniaritÄƒÈ›ii   |
| Elastic Net        | ~0.506    | Compromis bun L1+L2 â†’ **l1_ratio = 0.1**        |

Concluzia iniÈ›ialÄƒ: **Elastic Net** era cel mai echilibrat model Ã®ntre reducerea varianÈ›ei È™i menÈ›inerea performanÈ›ei.

---

## ğŸ§ª Experiment suplimentar: TimeSeriesSplit + grid rafinat

Pentru evaluare corectÄƒ pe date temporale a fost creat un notebook suplimentar:

elastic_net_timeseries_experiment.ipynb


### ğŸ” Rezultate cheie
- Elastic Net converge la **l1_ratio = 0.01**, deci â‰ˆ99% penalizare L2 â†’ comportament aproape identic cu **Ridge**.  
- Lasso eliminÄƒ **3 coeficienÈ›i**, Elastic Net eliminÄƒ **1**, Ridge nu eliminÄƒ nimic.  
- Curbele de Ã®nvÄƒÈ›are indicÄƒ stabilitate superioarÄƒ pentru metodele L2.  
- DistribuÈ›ia predicÈ›iilor (KDE) este aproape identicÄƒ pentru toate modelele.

### ğŸ” Interpretare
Datasetul este dominat de multicoliniaritate â†’ penalizarea **L2** este cea mai adecvatÄƒ.  
Elastic Net devine, Ã®n practicÄƒ, un model Ridge cu o micÄƒ componentÄƒ L1 pentru reducerea minimÄƒ a zgomotului.

---

## â–¶ï¸ Rulare notebook
Notebook-urile pot fi rulate Ã®n **Google Colab** Ã®ncÄƒrcÃ¢nd manual fiÈ™ierele `.ipynb`.  
Datasetul Kaggle trebuie Ã®ncÄƒrcat manual (nu este inclus Ã®n repository).

---

## ğŸ“ Structura repository-ului

â”œâ”€â”€ TEMA2.ipynb # Studiul principal (analiza iniÈ›ialÄƒ)
â”œâ”€â”€ elastic_net_timeseries_experiment.ipynb # Experiment suplimentar (TimeSeriesSplit)
â””â”€â”€ README.md # DocumentaÈ›ia proiectului


---

## ğŸ Concluzie

- Regresia liniarÄƒ oferÄƒ cel mai mic RMSE, dar este mai sensibilÄƒ la varianÈ›Äƒ.  
- Modelele L2 (Ridge, Elastic Net) sunt cele mai stabile pe date reale.  
- Elastic Net, cu validare temporalÄƒ, converge aproape complet cÄƒtre Ridge.  
- SHAP confirmÄƒ relevanÈ›a variabilelor PM10, DEWP, O3, NOâ‚‚ È™i TEMP.  

Acest proiect oferÄƒ o analizÄƒ completÄƒ, reproductibilÄƒ È™i interpretabilÄƒ a modului Ã®n care regularizarea influenÈ›eazÄƒ calitatea modelelor de regresie pe date reale.
