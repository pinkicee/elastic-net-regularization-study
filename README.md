# ğŸ“˜ Studiu de Caz â€“ Regularizare Ã®n Modele de Regresie  
**Elastic Net vs. Ridge vs. Lasso**

Acest repository conÈ›ine notebook-ul **TEMA2.ipynb**, Ã®n care este implementat un studiu comparativ al metodelor de regularizare utilizate pentru reducerea overfitting-ului Ã®n modelele de regresie.

Scopul proiectului este analiza modului Ã®n care tehnicile **L1 (Lasso)**, **L2 (Ridge)** È™i **Elastic Net** influenÈ›eazÄƒ performanÈ›a, stabilitatea È™i generalizarea unui model de regresie aplicat pe date reale.

---

## ğŸ§  ConÈ›inutul proiectului

Notebook-ul include:

### âœ” Pre-procesarea setului de date  
- Ã®ncÄƒrcarea datelor din proiectul Beijing Multi-Site Air Quality  
- curÄƒÈ›are È™i completare valori lipsÄƒ  
- transformÄƒri pentru variabile numerice È™i categorice  
- scalare standardizatÄƒ  
- transformarea logaritmicÄƒ a variabilei È›intÄƒ (PM2.5)  
- Ã®mpÄƒrÈ›irea datelor Train/Test fÄƒrÄƒ shuffle (date temporale)

### âœ” Implementarea modelelor  
Modelele antrenate Ã®n notebook:

- **Linear Regression** (baseline)
- **Ridge Regression (L2)**
- **Lasso Regression (L1)**
- **Elastic Net (L1+L2)**

Optimizarea hiperparametrilor a fost realizatÄƒ folosind **GridSearchCV**.

### âœ” Evaluare È™i analizÄƒ  
Notebook-ul conÈ›ine:

- RMSE pe seturile Train È™i Test  
- RÂ² pentru fiecare model  
- analiza biasâ€“varianÈ›Äƒ  
- curbe de Ã®nvÄƒÈ›are  
- distribuÈ›ia predicÈ›iilor (KDE)  
- comparaÈ›ia coeficienÈ›ilor È™i a coeficienÈ›ilor eliminaÈ›i  
- interpretare SHAP pentru modelul Elastic Net  

---

## ğŸ“Š Rezultate pe scurt

| Model | RMSE Test | ObservaÈ›ii |
|------|-----------|------------|
| **Linear Regression** | 0.5047 | Precizie numericÄƒ cea mai ridicatÄƒ, uÈ™or overfitting |
| **Lasso (L1)** | 0.5050 | EliminÄƒ 2 variabile, regularizare foarte slabÄƒ |
| **Ridge (L2)** | 0.5063 | Cel mai stabil Ã®n prezenÈ›a multicoliniaritÄƒÈ›ii |
| **Elastic Net** | 0.5064 | Cel mai bun compromis L1+L2, stabil È™i robust |

Elastic Net a identificat un **l1_ratio = 0.1**, ceea ce indicÄƒ un dataset dominat de multicoliniaritate, unde penalizarea L2 este mai potrivitÄƒ.

---

## â–¶ï¸ Rulare notebook

PoÈ›i rula notebook-ul Ã®n Google Colab Ã®ncÄƒrcÃ¢nd local fiÈ™ierul **TEMA2.ipynb**.

Datasetul necesitÄƒ Ã®ncÄƒrcare manualÄƒ (nu este inclus Ã®n repository).

---

## ğŸ“ FiÈ™iere Ã®n acest repository

â”œâ”€â”€ TEMA2.ipynb # Notebook complet cu cod, grafice È™i analizÄƒ
â””â”€â”€ README.md # DocumentaÈ›ia proiectului


---

## ğŸ“š ReferinÈ›e

- Hastie, Tibshirani & Friedman â€“ *The Elements of Statistical Learning*  
- Tibshirani â€“ *Lasso Regression* (1996)  
- Zou & Hastie â€“ *Elastic Net* (2005)  
- scikit-learn documentation  
- SHAP documentation  
- Beijing Air Quality Dataset â€“ Kaggle  

---

## ğŸ Concluzie

Proiectul demonstreazÄƒ cÄƒ:

- Regresia liniarÄƒ oferÄƒ cea mai micÄƒ eroare numericÄƒ,  
- Modelele regularizate sunt mai stabile Ã®n situaÈ›ii reale,  
- **Elastic Net** obÈ›ine cel mai bun compromis Ã®ntre reducerea varianÈ›ei È™i pÄƒstrarea performanÈ›ei,  
- Interpretarea SHAP confirmÄƒ consistenÈ›a È™i relevanÈ›a variabilelor.

Acest studiu oferÄƒ o analizÄƒ completÄƒ È™i reproductibilÄƒ a modului Ã®n care regularizarea influenÈ›eazÄƒ calitatea modelelor de regresie pe date reale.

